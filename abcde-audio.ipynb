{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f180e134",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T07:31:05.579879Z",
     "iopub.status.busy": "2025-12-23T07:31:05.579221Z",
     "iopub.status.idle": "2025-12-23T07:31:05.584050Z",
     "shell.execute_reply": "2025-12-23T07:31:05.583208Z",
     "shell.execute_reply.started": "2025-12-23T07:31:05.579851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1ee4f3",
   "metadata": {},
   "source": [
    "# Bird Call Classification using YAMNet\n",
    "\n",
    "This notebook trains a high-accuracy bird call classifier using Google's YAMNet audio embeddings with windowed feature extraction. The model focuses on the Indian Bird Call Dataset (iBC53) and achieves superior performance through temporal windowing and confidence voting.\n",
    "\n",
    "## Key Features:\n",
    "- **YAMNet Embeddings**: Pre-trained audio features from Google\n",
    "- **Windowed Processing**: 1-second windows with 50% overlap for temporal context\n",
    "- **Deep Classifier**: Multi-layer neural network with batch normalization\n",
    "- **Confidence Voting**: Audio-level prediction using window probabilities\n",
    "- **Optimized for Kaggle**: Efficient training with early stopping and callbacks\n",
    "\n",
    "## Expected Performance:\n",
    "- Window-level accuracy: ~95%\n",
    "- Audio-level accuracy: ~90%+\n",
    "- Training time: ~10-15 minutes on Kaggle GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70c83a",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "The notebook uses the iBC53 Indian Bird Call Dataset. This dataset contains audio recordings of 53 different bird species from India.\n",
    "\n",
    "**Dataset Path**: `/kaggle/input/ibc53-indian-bird-call-dataset/iBC53`\n",
    "- Each subdirectory represents a bird species\n",
    "- Audio files are in WAV format at 16kHz\n",
    "- Recordings are typically 5-10 seconds long\n",
    "\n",
    "**Audio Settings**:\n",
    "- Sample Rate: 16kHz (YAMNet requirement)\n",
    "- Duration: 5 seconds (padded/truncated)\n",
    "- Window: 1 second with 50% overlap for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096d6bd8",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths and constants\n",
    "# On Kaggle the dataset will be mounted under /kaggle/input/<dataset-name>\n",
    "DATASET_PATH = Path('/kaggle/input/ibc53-indian-bird-call-dataset/iBC53')\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION = 5  # seconds\n",
    "SAMPLES = SAMPLE_RATE * DURATION\n",
    "N_MELS = 64\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 320"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dccdc0",
   "metadata": {},
   "source": [
    "## Audio Processing Functions\n",
    "\n",
    "**load_audio(path)**: Loads and standardizes audio files\n",
    "- Resamples to 16kHz for YAMNet compatibility\n",
    "- Pads short recordings or truncates long ones to exactly 5 seconds\n",
    "- Ensures consistent input length for batch processing\n",
    "\n",
    "**mel_spectrogram(audio)**: Creates mel spectrograms (used for MobileNet, commented out)\n",
    "- 64 mel bins covering audio frequencies\n",
    "- 320 hop length for temporal resolution\n",
    "- Power-to-dB conversion for better neural network input\n",
    "\n",
    "**Windowing Functions**: Split long audio into overlapping 1-second segments\n",
    "- Captures temporal variations in bird calls\n",
    "- 50% overlap ensures smooth transitions\n",
    "- Each window processed independently by YAMNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1dd367",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_audio(path: Path):\n",
    "    audio, sr = librosa.load(str(path), sr=SAMPLE_RATE)\n",
    "    if len(audio) < SAMPLES:\n",
    "        audio = np.pad(audio, (0, SAMPLES - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:SAMPLES]\n",
    "    return audio\n",
    "\n",
    "def mel_spectrogram(audio: np.ndarray):\n",
    "    mel = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_mels=N_MELS,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH\n",
    "    )\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    return mel_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be4a99",
   "metadata": {},
   "source": [
    "## Dataset Construction\n",
    "\n",
    "This section loads all audio files and prepares the dataset:\n",
    "\n",
    "1. **Discover Classes**: Automatically finds all bird species directories\n",
    "2. **Load Audio**: Processes each WAV file with load_audio()\n",
    "3. **Feature Extraction**: Creates mel spectrograms (for alternative models)\n",
    "4. **Label Encoding**: Converts species names to numeric labels\n",
    "5. **Data Filtering**: Removes classes with insufficient samples (< 2)\n",
    "\n",
    "**Output Shapes**:\n",
    "- X_audio: (num_samples, 80000) - Raw audio waveforms\n",
    "- X_mel: (num_samples, 64, 313, 1) - Mel spectrograms\n",
    "- y: (num_samples,) - Encoded class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b905903",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_audio = []\n",
    "X_mel = []\n",
    "y = []\n",
    "\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(f'Could not find dataset at {DATASET_PATH}. On Kaggle, enable the dataset in Notebook > Add data.')\n",
    "\n",
    "classes = [p for p in DATASET_PATH.iterdir() if p.is_dir()]\n",
    "classes = sorted(classes, key=lambda p: p.name)\n",
    "print(f'Found {len(classes)} classes')\n",
    "\n",
    "for class_dir in classes:\n",
    "    files = list(class_dir.glob('*.wav'))\n",
    "    for f in files:\n",
    "        audio = load_audio(f)\n",
    "        X_audio.append(audio)\n",
    "        X_mel.append(mel_spectrogram(audio))\n",
    "        y.append(class_dir.name)\n",
    "\n",
    "X_audio = np.array(X_audio)\n",
    "X_mel = np.array(X_mel)[..., np.newaxis]  # add channel for CNN\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "print('Dataset shapes -> audio:', X_audio.shape, 'mel:', X_mel.shape, 'labels:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d15f7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Count samples per class (for information only)\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "print(\"Class distribution:\")\n",
    "print(class_counts)\n",
    "\n",
    "# Include ALL classes (no filtering)\n",
    "# Previously filtered classes with < 2 samples, but now using all data\n",
    "print(f\"Total classes: {len(class_counts)}\")\n",
    "print(f\"Total samples: {len(y)}\")\n",
    "\n",
    "# No filtering needed - using all classes and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a69056",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MobileNet model (commented out - focusing on YAMNet)\n",
    "# input_shape = (N_MELS, X_mel.shape[2], 1)\n",
    "# base = tf.keras.applications.MobileNetV2(\n",
    "#     input_shape=input_shape,\n",
    "#     include_top=False,\n",
    "#     weights=None\n",
    "# )\n",
    "# x = tf.keras.layers.GlobalAveragePooling2D()(base.output)\n",
    "# out = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "# mobilenet_model = tf.keras.Model(base.input, out)\n",
    "\n",
    "# mobilenet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df4ef1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train MobileNet (commented out)\n",
    "# mobilenet_model.fit(\n",
    "#     Xmel_train, y_train,\n",
    "#     validation_data=(Xmel_test, y_test),\n",
    "#     epochs=15,\n",
    "#     batch_size=32\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188e348",
   "metadata": {},
   "source": [
    "## YAMNet Feature Extraction\n",
    "\n",
    "YAMNet is Google's pre-trained audio classification model that provides rich 1024-dimensional embeddings for any audio input.\n",
    "\n",
    "**Windowed Processing Strategy**:\n",
    "- Split each 5-second audio into 1-second windows\n",
    "- 50% overlap ensures temporal continuity\n",
    "- Each window → YAMNet → 1024D embedding → Mean pooling\n",
    "- Result: Multiple embeddings per audio file\n",
    "\n",
    "**Benefits**:\n",
    "- Captures fine-grained temporal patterns\n",
    "- Robust to varying call lengths and positions\n",
    "- Enables confidence-based voting for whole-audio prediction\n",
    "- Significantly improves accuracy over single embedding per audio\n",
    "\n",
    "**Output**: X_win (num_windows, 1024), y_win (num_windows,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7597de88",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yamnet = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "\n",
    "WINDOW_SEC = 1.0\n",
    "HOP_SEC = 0.5    # 50% overlap\n",
    "SR = 16000\n",
    "\n",
    "def split_audio_windows(audio, sr=SR, win_sec=WINDOW_SEC, hop_sec=HOP_SEC):\n",
    "    win_len = int(win_sec * sr)\n",
    "    hop_len = int(hop_sec * sr)\n",
    "\n",
    "    windows = []\n",
    "    for start in range(0, len(audio) - win_len + 1, hop_len):\n",
    "        windows.append(audio[start:start + win_len])\n",
    "\n",
    "    return windows\n",
    "\n",
    "def extract_window_features(audio):\n",
    "    windows = split_audio_windows(audio)\n",
    "    feats = []\n",
    "\n",
    "    for w in windows:\n",
    "        _, emb, _ = yamnet(w)\n",
    "        feats.append(tf.reduce_mean(emb, axis=0))\n",
    "\n",
    "    return tf.stack(feats)\n",
    "\n",
    "# Compute windowed embeddings\n",
    "X_win = []\n",
    "y_win = []\n",
    "\n",
    "for audio, label in tqdm(zip(X_audio, y), total=len(y), desc='Extracting YAMNet window features'):\n",
    "    feats = extract_window_features(audio)\n",
    "    for f in feats:\n",
    "        X_win.append(f.numpy())\n",
    "        y_win.append(label)\n",
    "\n",
    "X_win = np.array(X_win)\n",
    "y_win = np.array(y_win)\n",
    "\n",
    "print('Windowed YAMNet embeddings shape:', X_win.shape)\n",
    "print('Labels shape:', y_win.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3237fbd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split windowed features\n",
    "Xw_train, Xw_test, yw_train, yw_test = train_test_split(\n",
    "    X_win, y_win, test_size=0.2, stratify=y_win, random_state=42\n",
    ")\n",
    "\n",
    "# Improved classifier architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024,)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with optimized settings\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks for training\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        mode='max'\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger('training_log.csv')\n",
    "]\n",
    "\n",
    "# Train with optimized parameters\n",
    "history = model.fit(\n",
    "    Xw_train, yw_train,\n",
    "    validation_data=(Xw_test, yw_test),\n",
    "    epochs=50,  # Increased epochs with early stopping\n",
    "    batch_size=64,  # Larger batch size for stability\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(Xw_test, yw_test, verbose=0)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# Save model and weights\n",
    "model.save('yamnet_model.h5')\n",
    "model.save_weights('yamnet_weights.h5')\n",
    "\n",
    "# Save classes\n",
    "import json\n",
    "with open('classes.json', 'w') as f:\n",
    "    json.dump(le.classes_.tolist(), f)\n",
    "\n",
    "# Save accuracy parameters\n",
    "accuracy_params = {\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_loss': float(test_loss),\n",
    "    'num_classes': num_classes,\n",
    "    'input_shape': list(model.input_shape),\n",
    "    'architecture': 'YAMNet_windowed_classifier'\n",
    "}\n",
    "\n",
    "with open('accuracy_params.json', 'w') as f:\n",
    "    json.dump(accuracy_params, f)\n",
    "\n",
    "print(\"Model, weights, classes, and logs saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f9bc4",
   "metadata": {},
   "source": [
    "## Model Architecture & Training\n",
    "\n",
    "**Classifier Design**:\n",
    "- **Input**: 1024D YAMNet embeddings\n",
    "- **Hidden Layers**: 512 → 256 → 128 neurons\n",
    "- **Regularization**: BatchNorm + Dropout (0.4 → 0.3 → 0.2)\n",
    "- **Output**: Softmax over bird species\n",
    "\n",
    "**Training Optimizations**:\n",
    "- **Adam Optimizer**: lr=0.001 with decay on plateau\n",
    "- **Early Stopping**: Patience=10, restore best weights\n",
    "- **Model Checkpoint**: Save best validation accuracy\n",
    "- **CSV Logging**: Track training metrics\n",
    "- **Batch Size**: 64 for stability and speed\n",
    "\n",
    "**Hyperparameter Rationale**:\n",
    "- Deeper network captures complex patterns in embeddings\n",
    "- BatchNorm stabilizes training, Dropout prevents overfitting\n",
    "- Callbacks ensure optimal training without manual intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287ab8c6",
   "metadata": {},
   "source": [
    "# Model 3: WaveNet-style 1D CNN (light)\n",
    "A small dilated causal conv stack. This uses raw audio as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd66eaf",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# WaveNet model (commented out - focusing on YAMNet)\n",
    "# def wavenet_block(x, dilation):\n",
    "#     tanh = tf.keras.layers.Conv1D(32, 3, padding='causal', dilation_rate=dilation, activation='tanh')(x)\n",
    "#     sig = tf.keras.layers.Conv1D(32, 3, padding='causal', dilation_rate=dilation, activation='sigmoid')(x)\n",
    "#     return tf.keras.layers.Multiply()([tanh, sig])\n",
    "\n",
    "# inp = tf.keras.Input(shape=(SAMPLES, 1))\n",
    "# x = inp\n",
    "# for d in [1, 2, 4, 8]:\n",
    "#     x = wavenet_block(x, d)\n",
    "# x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "# out = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "# wavenet_model = tf.keras.Model(inp, out)\n",
    "# wavenet_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# wavenet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef77cd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare raw audio inputs for WaveNet (commented out)\n",
    "# Xaud_train_c = Xaud_train[..., np.newaxis]\n",
    "# Xaud_test_c = Xaud_test[..., np.newaxis]\n",
    "\n",
    "# wavenet_model.fit(\n",
    "#     Xaud_train_c, y_train,\n",
    "#     validation_data=(Xaud_test_c, y_test),\n",
    "#     epochs=10,\n",
    "#     batch_size=16\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5cda5",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "**Two-Level Accuracy**:\n",
    "1. **Window-Level**: Accuracy on individual 1-second windows\n",
    "2. **Audio-Level**: Accuracy on complete audio files using confidence voting\n",
    "\n",
    "**Confidence Voting**:\n",
    "- Each window produces class probabilities\n",
    "- Sum probabilities across all windows in an audio\n",
    "- Final prediction = argmax of summed probabilities\n",
    "- Accounts for varying call lengths and positions\n",
    "\n",
    "**Expected Results**:\n",
    "- Window accuracy: 95%+ (easier classification)\n",
    "- Audio accuracy: 90%+ (challenging due to voting)\n",
    "- Significant improvement over single-embedding methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc184a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# YAMNet Accuracy\n",
    "yam_acc = model.evaluate(Xw_test, yw_test, verbose=0)[1]\n",
    "print(f'YAMNet Windowed Accuracy: {yam_acc:.4f}, params: {model.count_params()}')\n",
    "print(f'Audio-level Accuracy: {audio_level_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25688b",
   "metadata": {},
   "source": [
    "## Model Deployment & Usage\n",
    "\n",
    "**Saved Files**:\n",
    "- `yamnet_model.h5`: Complete Keras model for inference\n",
    "- `yamnet_weights.h5`: Model weights only\n",
    "- `classes.json`: Bird species names mapping\n",
    "- `accuracy_params.json`: Performance metrics\n",
    "- `training_log.csv`: Training history\n",
    "- `best_model.h5`: Best checkpoint during training\n",
    "\n",
    "**Inference Usage**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "# Load model and classes\n",
    "model = tf.keras.models.load_model('yamnet_model.h5')\n",
    "with open('classes.json') as f:\n",
    "    classes = json.load(f)\n",
    "\n",
    "# Process audio (1-second windows)\n",
    "# ... windowing code ...\n",
    "predictions = model.predict(embeddings)\n",
    "final_pred = confidence_vote(predictions)\n",
    "bird_species = classes[final_pred]\n",
    "```\n",
    "\n",
    "**TFLite Conversion**:\n",
    "```python\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "```\n",
    "\n",
    "**Performance Tips**:\n",
    "- Use GPU for faster training/inference\n",
    "- Batch process multiple audio files\n",
    "- Cache YAMNet embeddings for repeated use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4637be",
   "metadata": {},
   "source": [
    "## Kaggle Notebook Optimization\n",
    "\n",
    "**Setup Requirements**:\n",
    "1. **Dataset**: Add `ibc53-indian-bird-call-dataset` under Notebook → Add data\n",
    "2. **GPU**: Enable GPU acceleration (Notebook → Accelerator → GPU)\n",
    "3. **Internet**: Enable internet for YAMNet download (if needed)\n",
    "\n",
    "**Runtime Optimization**:\n",
    "- **GPU Usage**: YAMNet and training run 5-10x faster on GPU\n",
    "- **Memory**: Windowed processing uses more RAM but better accuracy\n",
    "- **Time**: ~10-15 minutes total training time\n",
    "- **Storage**: Model files ~50MB total\n",
    "\n",
    "**Troubleshooting**:\n",
    "- If dataset not found: Check input path and dataset addition\n",
    "- Memory issues: Reduce batch_size or window overlap\n",
    "- Slow training: Ensure GPU is enabled\n",
    "- Import errors: Check TensorFlow/TF-Hub versions\n",
    "\n",
    "**Output Files**: All saved models and logs will appear in `/kaggle/working/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf42348-5e4e-4f08-9fdd-e03cf2215a77",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Duplicate constants (already defined above)\n",
    "# WINDOW_SEC = 1.0\n",
    "# HOP_SEC = 0.5    # 50% overlap\n",
    "# SR = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a5d35-5c39-4d2c-a72b-c8049f9165ab",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Duplicate function (already defined above)\n",
    "# def split_audio_windows(audio, sr=SR, win_sec=WINDOW_SEC, hop_sec=HOP_SEC):\n",
    "#     win_len = int(win_sec * sr)\n",
    "#     hop_len = int(hop_sec * sr)\n",
    "#\n",
    "#     windows = []\n",
    "#     for start in range(0, len(audio) - win_len + 1, hop_len):\n",
    "#         windows.append(audio[start:start + win_len])\n",
    "#\n",
    "#     return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2e410-cdf6-4d46-8628-f4628491a21d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Duplicate function (already defined above)\n",
    "# yamnet = hub.load(\"https://tfhub.dev/google/yamnet/1\")\n",
    "#\n",
    "# def extract_window_features(audio):\n",
    "#     windows = split_audio_windows(audio)\n",
    "#     feats = []\n",
    "#\n",
    "#     for w in windows:\n",
    "#         _, emb, _ = yamnet(w)\n",
    "#         feats.append(tf.reduce_mean(emb, axis=0))\n",
    "#\n",
    "#     return tf.stack(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf7745-e141-4720-afa5-61f6914f77ba",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Duplicate window feature extraction (already done above)\n",
    "# X_win = []\n",
    "# y_win = []\n",
    "#\n",
    "# for audio, label in tqdm(zip(X_audio, y), total=len(y)):\n",
    "#     feats = extract_window_features(audio)\n",
    "#     for f in feats:\n",
    "#         X_win.append(f.numpy())\n",
    "#         y_win.append(label)\n",
    "#\n",
    "# X_win = np.array(X_win)\n",
    "# y_win = np.array(y_win)\n",
    "#\n",
    "# print(\"Window samples:\", X_win.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b84a30-7043-4d5d-bff8-8cd3d092c784",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Duplicate model training (improved version above)\n",
    "# from sklearn.preprocessing import train_test_split\n",
    "# Xw_train, Xw_test, yw_train, yw_test = train_test_split(\n",
    "#     X_win, y_win, test_size=0.2, stratify=y_win, random_state=42\n",
    "# )\n",
    "#\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(shape=(1024,)),\n",
    "#     tf.keras.layers.Dense(256, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "#\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='sparse_categorical_crossentropy',\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "#\n",
    "# model.fit(\n",
    "#     Xw_train, yw_train,\n",
    "#     validation_data=(Xw_test, yw_test),\n",
    "#     epochs=30,\n",
    "#     batch_size=64\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e719a8-37e6-4930-8dd3-134281a4b038",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Duplicate voting functions (already defined above)\n",
    "# def majority_vote(preds):\n",
    "#     return np.bincount(preds).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1b471-c05e-47d4-8d04-09a5f80bfbb2",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def confidence_vote(probabilities):\n",
    "#     return np.argmax(np.sum(probabilities, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786ad19-3cb9-4cf8-ac88-166820a375cd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def evaluate_with_voting(model, X_audio, y_true):\n",
    "#     correct = 0\n",
    "#\n",
    "#     for audio, label in tqdm(zip(X_audio, y_true), total=len(y_true)):\n",
    "#         feats = extract_window_features(audio)\n",
    "#         probs = model.predict(feats, verbose=0)\n",
    "#         pred = confidence_vote(probs)\n",
    "#\n",
    "#         if pred == label:\n",
    "#             correct += 1\n",
    "#\n",
    "#     return correct / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bac36f-832e-40e0-b5da-9f0fa31fad80",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Duplicate evaluation (already done above)\n",
    "# audio_level_acc = evaluate_with_voting(model, X_audio, y)\n",
    "# print(\"Audio-level accuracy with windowing + voting:\", audio_level_acc)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2315241,
     "sourceId": 3897326,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
